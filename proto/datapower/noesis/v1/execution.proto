// Execution Plan Protocol
// ------------------------
// Defines the execution plan generated by the pipeline planner (runner.Plan()).
// The planner analyzes the pipeline definition, validates SQL against schemas,
// determines the execution strategy, and generates job specifications.
//
// Execution Strategies:
// - BATCH_ONLY: All fields are immutable, only batch reconciliation needed
// - STREAMING_ONLY: All fields are mutable, only streaming jobs needed
// - HYBRID: Mix of mutable (streaming) and immutable (batch) fields
//
// The ExecutionPlan is consumed by the RunPipeline workflow to orchestrate execution.

syntax = "proto3";

package datapower.noesis.v1;

option go_package = "github.com/data-power-io/noesis-protocol/languages/go/datapower/noesis/v1;noesisv1";
option java_package = "datapower.noesis.v1";
option java_multiple_files = true;

import "datapower/noesis/v1/pipeline.proto";

// ExecutionPlan is the output of the planning phase
// It contains all information needed to execute the pipeline
message ExecutionPlan {
  // Pipeline name
  string pipeline_name = 1;

  // Determined execution strategy
  ExecutionStrategy strategy = 2;

  // Streaming job specifications (empty for BATCH_ONLY)
  repeated StreamingJobSpec streaming_jobs = 3;

  // Batch source specifications
  repeated BatchSourceSpec batch_sources = 4;

  // Batch reconciliation job specification (may be null for STREAMING_ONLY)
  BatchReconciliationJobSpec batch_reconciliation = 5;

  // Planning metadata
  PlanningMetadata metadata = 6;

  // The original pipeline definition (for reference)
  PipelineDefinition pipeline_definition = 7;
}

enum ExecutionStrategy {
  STRATEGY_UNSPECIFIED = 0;

  // All target fields are immutable, only batch processing needed
  // No streaming jobs are created
  BATCH_ONLY = 1;

  // All target fields are mutable and come from streaming sources
  // Only streaming jobs are created, no batch reconciliation
  STREAMING_ONLY = 2;

  // Mix of mutable (from streaming sources) and immutable (from batch sources)
  // Both streaming jobs AND batch reconciliation are created
  HYBRID = 3;
}

// StreamingJobSpec defines a Flink streaming job for real-time updates
// One streaming job is created per streaming source that contributes to mutable fields
message StreamingJobSpec {
  // Unique job name (e.g., "customer-360-view_orders_patcher")
  string job_name = 1;

  // Source alias from pipeline definition
  string source_alias = 2;

  // Kafka topic to consume from
  string kafka_topic = 3;

  // Target fields that this job updates (all must be mutable=true)
  repeated string mutable_fields = 4;

  // Sink topic where patch events are written
  string sink_topic = 5;

  // Generated Flink SQL for this streaming job
  string flink_sql = 6;

  // Primary key field for grouping
  string primary_key = 7;

  // Kafka consumer group ID
  string consumer_group = 8;

  // State backend configuration
  StateBackendConfig state_backend = 9;

  // Checkpointing configuration
  CheckpointConfig checkpoint = 10;

  // Parallelism for this job
  int32 parallelism = 11;
}

// StateBackendConfig defines state storage for streaming jobs
message StateBackendConfig {
  // State backend type (rocksdb, memory, filesystem)
  string type = 1;

  // For RocksDB: incremental checkpoints
  bool incremental_checkpoints = 2;

  // Storage path (for persistent backends)
  string storage_path = 3;
}

// CheckpointConfig defines checkpointing behavior
message CheckpointConfig {
  // Checkpoint interval in milliseconds
  int64 interval_ms = 1;

  // Checkpoint mode (exactly_once, at_least_once)
  string mode = 2;

  // Checkpoint timeout in milliseconds
  int64 timeout_ms = 3;

  // Min pause between checkpoints in milliseconds
  int64 min_pause_ms = 4;

  // Max concurrent checkpoints
  int32 max_concurrent = 5;
}

// BatchSourceSpec defines how to extract and load a batch source
message BatchSourceSpec {
  // Source alias from pipeline definition
  string source_alias = 1;

  // Target Iceberg table name
  string iceberg_table = 2;

  // Whether this source needs extraction (false if already in Iceberg)
  bool needs_extraction = 3;

  // Connector configuration (if needs_extraction=true)
  ConnectorConfig connector = 4;

  // Desired parallelism for extraction
  int32 parallelism = 5;

  // Schema for the source
  repeated SchemaField schema = 6;

  // Estimated row count (for planning)
  int64 estimated_rows = 7;
}

// ConnectorConfig holds connector-specific configuration
message ConnectorConfig {
  // Connector type (e.g., "postgres", "mysql", "oracle")
  string connector_type = 1;

  // Docker image for the connector
  string connector_image = 2;

  // Connector version
  string connector_version = 3;

  // Connection configuration (credentials, endpoints, etc.)
  map<string, string> config = 4;

  // Entity name in source system
  string entity_name = 5;
}

// BatchReconciliationJobSpec defines the main batch transformation job
message BatchReconciliationJobSpec {
  // Unique job name (e.g., "customer-360-view_batch_reconciliation")
  string job_name = 1;

  // Sink topic where full-state events are written
  string sink_topic = 2;

  // Generated Flink SQL for batch reconciliation
  // This wraps the user's original SQL query
  string flink_sql = 3;

  // Source Iceberg tables referenced in the job
  repeated string source_tables = 4;

  // Primary key for output records
  string primary_key = 5;

  // Parallelism for the batch job
  int32 parallelism = 6;

  // Expected runtime (for monitoring)
  int64 expected_runtime_ms = 7;
}

// ExtractionSplit represents a partition of data to be extracted
// Returned by the PlanBatchSourceExtractionActivity
message ExtractionSplit {
  // Unique split identifier (e.g., "split-0", "split-1")
  string split_id = 1;

  // Source alias this split belongs to
  string source_alias = 2;

  // Filter condition for this split (e.g., "WHERE id BETWEEN 1 AND 1000000")
  string filter_condition = 3;

  // Estimated rows in this split
  int64 estimated_rows = 4;

  // Connector configuration
  ConnectorConfig connector = 5;

  // Output configuration
  OutputConfig output = 6;
}

// OutputConfig defines where extracted data is written
message OutputConfig {
  // Object storage bucket (e.g., "gs://my-bucket", "s3://my-bucket")
  string bucket = 1;

  // Path within bucket (e.g., "staging/customers_batch/run-xyz/partition-1")
  string path = 2;

  // Parquet file format options
  ParquetOptions parquet = 3;
}

message ParquetOptions {
  // Compression codec (snappy, gzip, lzo, zstd)
  string compression = 1;

  // Row group size
  int64 row_group_size = 2;

  // Page size
  int64 page_size = 3;
}

// PlanningMetadata holds information about the planning process
message PlanningMetadata {
  // When planning occurred
  int64 planned_at_epoch_ms = 1;

  // Who/what initiated planning
  string planned_by = 2;

  // Planning duration in milliseconds
  int64 planning_duration_ms = 3;

  // SQL analysis results
  SQLAnalysisResult sql_analysis = 4;

  // Strategy selection reasoning
  string strategy_reason = 5;
}

// SQLAnalysisResult captures the results of SQL query analysis
message SQLAnalysisResult {
  // Whether SQL is valid
  bool valid = 1;

  // Parse errors (if any)
  repeated string parse_errors = 2;

  // Referenced source aliases
  repeated string referenced_sources = 3;

  // Referenced columns per source
  map<string, ColumnList> source_columns = 4;

  // Output columns (SELECT list)
  repeated string output_columns = 5;

  // Query type (SELECT, JOIN, AGGREGATION, WINDOW)
  string query_type = 6;

  // Has GROUP BY clause
  bool has_grouping = 7;

  // Has window functions
  bool has_windowing = 8;

  // Join types used (INNER, LEFT, RIGHT, FULL)
  repeated string join_types = 9;
}

message ColumnList {
  repeated string columns = 1;
}

// ExecutionMode determines how the workflow executes
enum ExecutionMode {
  EXECUTION_MODE_UNSPECIFIED = 0;

  // First-time deployment: deploy streaming jobs, then run batch
  INITIAL_DEPLOYMENT = 1;

  // Scheduled batch run: skip streaming job deployment
  SCHEDULED_RUN = 2;

  // Manual trigger: can be either mode depending on context
  MANUAL = 3;
}

// ExecutionResult captures the outcome of pipeline execution
message ExecutionResult {
  // Pipeline name
  string pipeline_name = 1;

  // Execution ID (workflow run ID)
  string execution_id = 2;

  // Execution mode
  ExecutionMode mode = 3;

  // Overall status
  ExecutionStatus status = 4;

  // Start and end times
  int64 started_at_epoch_ms = 5;
  int64 completed_at_epoch_ms = 6;

  // Duration in milliseconds
  int64 duration_ms = 7;

  // Streaming job results
  repeated StreamingJobResult streaming_job_results = 8;

  // Batch extraction results
  repeated ExtractionResult extraction_results = 9;

  // Batch reconciliation result
  BatchJobResult batch_result = 10;

  // Error information (if failed)
  string error_message = 11;
  string error_stack_trace = 12;

  // Metrics
  ExecutionMetrics metrics = 13;
}

enum ExecutionStatus {
  EXECUTION_STATUS_UNSPECIFIED = 0;
  RUNNING = 1;
  COMPLETED = 2;
  FAILED = 3;
  CANCELLED = 4;
}

message StreamingJobResult {
  string job_name = 1;
  string flink_job_id = 2;
  bool deployed_successfully = 3;
  string error_message = 4;
  int64 deployment_time_ms = 5;
}

message ExtractionResult {
  string split_id = 1;
  string source_alias = 2;
  bool success = 3;
  int64 rows_extracted = 4;
  int64 bytes_written = 5;
  int64 duration_ms = 6;
  string error_message = 7;
}

message BatchJobResult {
  string job_name = 1;
  string flink_job_id = 2;
  bool success = 3;
  int64 records_processed = 4;
  int64 records_written = 5;
  int64 duration_ms = 6;
  string error_message = 7;
}

message ExecutionMetrics {
  // Total records processed
  int64 total_records = 1;

  // Total bytes processed
  int64 total_bytes = 2;

  // Extraction metrics
  int64 extraction_duration_ms = 3;
  int64 extraction_records = 4;

  // Iceberg load metrics
  int64 iceberg_load_duration_ms = 5;
  int64 iceberg_records_loaded = 6;

  // Batch job metrics
  int64 batch_job_duration_ms = 7;
  int64 batch_records_written = 8;

  // Parallelism achieved
  int32 extraction_parallelism = 9;
  int32 batch_parallelism = 10;
}
